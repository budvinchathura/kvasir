{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "transfer.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/budvinchathura/kvasir/blob/master/vgg16_completed.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sDjx5qsBkBb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ! wget 'https://datasets.simula.no/kvasir/data/kvasir-dataset-v2.zip'\n",
        "# ! wget -P 'drive/My Drive/kvasir_project/' 'https://datasets.simula.no/kvasir/data/kvasir-dataset-v2.zip'\n",
        "\n",
        "# ! unzip 'kvasir-dataset-v2.zip'\n",
        "# ! unzip 'drive/My Drive/kvasir_project/kvasir-dataset-v2.zip'\n",
        "# ! cp -R 'kvasir-dataset-v2' 'drive/My Drive/kvasir_project'\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive',force_remount=True)\n",
        "\n",
        "# ! unzip 'drive/My Drive/DatasetZip/test-Set.zip' -d \"./\"\n",
        "# ! unzip 'drive/My Drive/DatasetZip/train-Set.zip' -d \"./\"\n",
        "# ! mkdir \"checkpoint\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1NULf6FY15Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmiDu5abBocO",
        "colab_type": "code",
        "outputId": "fe021603-d1c6-4408-bc24-6554eaed79dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        }
      },
      "source": [
        "HEIGHT = 150\n",
        "WIDTH = 150\n",
        "\n",
        "# from keras.applications.resnet50 import ResNet50, preprocess_input\n",
        "# base_model = ResNet50(weights='imagenet', \n",
        "#                       include_top=False, \n",
        "#                       input_shape=(HEIGHT, WIDTH, 3))\n",
        "\n",
        "\n",
        "from keras.applications.vgg16 import VGG16, preprocess_input\n",
        "base_model = VGG16(weights='imagenet', \n",
        "                      include_top=False, \n",
        "                      input_shape=(HEIGHT, WIDTH, 3))\n",
        "\n",
        "\n",
        "# from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
        "# base_model = InceptionV3(weights='imagenet', \n",
        "#                       include_top=False, \n",
        "#                       input_shape=(HEIGHT, WIDTH, 3))\n",
        "\n",
        "\n",
        "# from keras.applications.resnet_v2 import ResNet50V2, preprocess_input\n",
        "# base_model = ResNet50V2(weights='imagenet', \n",
        "#                       include_top=False, \n",
        "#                       input_shape=(HEIGHT, WIDTH, 3))\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjLxPbMuBxfh",
        "colab_type": "code",
        "outputId": "3352c07f-c9fd-4737-a00d-6b4d604be55c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "TRAIN_DIR = \"train-Set\"\n",
        "TEST_DIR = \"test-Set\"\n",
        "HEIGHT = 150\n",
        "WIDTH = 150\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# train_datagen =  ImageDataGenerator(\n",
        "#       preprocessing_function=preprocess_input,\n",
        "#       rotation_range=90,\n",
        "#       horizontal_flip=True,\n",
        "#       vertical_flip=True\n",
        "#     )\n",
        "\n",
        "# train_generator = train_datagen.flow_from_directory(TRAIN_DIR, \n",
        "#                                                     target_size=(HEIGHT, WIDTH), \n",
        "#                                                     batch_size=BATCH_SIZE)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train_datagen =  ImageDataGenerator(\n",
        "      preprocessing_function=preprocess_input)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(TRAIN_DIR, \n",
        "                                                    target_size=(HEIGHT, WIDTH), \n",
        "                                                    batch_size=BATCH_SIZE)\n",
        "\n",
        "\n",
        "test_datagen =  ImageDataGenerator(\n",
        "      preprocessing_function=preprocess_input)\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(TEST_DIR, \n",
        "                                                    target_size=(HEIGHT, WIDTH), \n",
        "                                                    batch_size=BATCH_SIZE)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 6400 images belonging to 8 classes.\n",
            "Found 1600 images belonging to 8 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLyp9P-kB3it",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "ce42e077-e951-4d45-fe47-1fe8ad47c8ec"
      },
      "source": [
        "from keras.layers import Dense, Activation, Flatten, Dropout\n",
        "from keras.models import Sequential, Model\n",
        "\n",
        "def build_finetune_model(base_model, dropout, fc_layers, num_classes):\n",
        "    for layer in base_model.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    x = base_model.output\n",
        "    x = Flatten()(x)\n",
        "    for fc in fc_layers:\n",
        "        # New FC layer, random init\n",
        "        x = Dense(fc, activation='relu')(x) \n",
        "        x = Dropout(dropout)(x)\n",
        "\n",
        "    # New softmax layer\n",
        "    predictions = Dense(num_classes, activation='softmax')(x) \n",
        "    \n",
        "    finetune_model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "    return finetune_model\n",
        "\n",
        "class_list = [\"dyed-lifted-polyps\", \"dyed-resection-margins\", \"esophagitis\",\"normal-cecum\",\"normal-pylorus\",\"normal-z-line\",\"polyps\",\"ulcerative-colitis\"]\n",
        "FC_LAYERS = [64,1024,64]\n",
        "dropout = 0.5\n",
        "\n",
        "finetune_model = build_finetune_model(base_model, \n",
        "                                      dropout=dropout, \n",
        "                                      fc_layers=FC_LAYERS, \n",
        "                                      num_classes=len(class_list))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVBf3aBXB79S",
        "colab_type": "code",
        "outputId": "c58c4583-c962-46e4-daf6-6daac093d2c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.optimizers import SGD, Adam\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "NUM_EPOCHS = 70\n",
        "BATCH_SIZE = 32\n",
        "num_train_images = 6400\n",
        "num_test_images = 1600\n",
        "\n",
        "adam = Adam(lr=0.0001)\n",
        "finetune_model.compile(adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "filepath=\"./checkpoints/\" + \"ResNet50\" + \"_model_weights.h5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor=[\"acc\"], verbose=1, mode='max')\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "print(finetune_model.summary())\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 150, 150, 3)       0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 150, 150, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 150, 150, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 75, 75, 64)        0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 75, 75, 128)       73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 75, 75, 128)       147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 37, 37, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 37, 37, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 37, 37, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 37, 37, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 18, 18, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 18, 18, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 9, 9, 512)         0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 8192)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 64)                524352    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1024)              66560     \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 64)                65600     \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 8)                 520       \n",
            "=================================================================\n",
            "Total params: 15,371,720\n",
            "Trainable params: 657,032\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdE5AS3T_UBn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "407a2385-7026-4490-a8e0-e7f7b2376839"
      },
      "source": [
        "history = finetune_model.fit_generator(train_generator, epochs=NUM_EPOCHS, workers=8, \n",
        "                                       steps_per_epoch=num_train_images // BATCH_SIZE, \n",
        "                                       shuffle=True,validation_data=test_generator,validation_steps=num_test_images//BATCH_SIZE)\n",
        "\n",
        "\n",
        "\n",
        "# Plot the training and validation loss + accuracy\n",
        "def plot_training(history):\n",
        "    acc = history.history['acc']\n",
        "    val_acc = history.history['val_acc']\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "    epochs = range(len(acc))\n",
        "\n",
        "    plt.plot(epochs, acc, 'r.')\n",
        "    plt.plot(epochs, val_acc, 'r')\n",
        "    plt.title('Training and validation accuracy')\n",
        "\n",
        "    # plt.figure()\n",
        "    # plt.plot(epochs, loss, 'r.')\n",
        "    # plt.plot(epochs, val_loss, 'r-')\n",
        "    # plt.title('Training and validation loss')\n",
        "    plt.show()\n",
        "\n",
        "    plt.savefig('acc_vs_epochs.png')\n",
        "\n",
        "plot_training(history)\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Epoch 1/70\n",
            "200/200 [==============================] - 118s 590ms/step - loss: 3.1334 - acc: 0.1338 - val_loss: 2.0247 - val_acc: 0.2263\n",
            "Epoch 2/70\n",
            "200/200 [==============================] - 111s 557ms/step - loss: 2.0609 - acc: 0.1514 - val_loss: 1.9507 - val_acc: 0.2450\n",
            "Epoch 3/70\n",
            "200/200 [==============================] - 112s 559ms/step - loss: 1.9955 - acc: 0.1909 - val_loss: 1.8629 - val_acc: 0.3075\n",
            "Epoch 4/70\n",
            "200/200 [==============================] - 111s 555ms/step - loss: 1.9090 - acc: 0.2417 - val_loss: 1.5935 - val_acc: 0.4313\n",
            "Epoch 5/70\n",
            "200/200 [==============================] - 110s 552ms/step - loss: 1.7333 - acc: 0.3169 - val_loss: 1.2796 - val_acc: 0.5613\n",
            "Epoch 6/70\n",
            "200/200 [==============================] - 110s 549ms/step - loss: 1.6216 - acc: 0.3769 - val_loss: 1.1867 - val_acc: 0.5856\n",
            "Epoch 7/70\n",
            "200/200 [==============================] - 110s 549ms/step - loss: 1.5141 - acc: 0.4236 - val_loss: 1.1058 - val_acc: 0.6244\n",
            "Epoch 8/70\n",
            "200/200 [==============================] - 110s 550ms/step - loss: 1.4345 - acc: 0.4491 - val_loss: 1.0375 - val_acc: 0.6975\n",
            "Epoch 9/70\n",
            "200/200 [==============================] - 110s 549ms/step - loss: 1.3683 - acc: 0.4717 - val_loss: 0.9375 - val_acc: 0.7350\n",
            "Epoch 10/70\n",
            "200/200 [==============================] - 110s 549ms/step - loss: 1.2771 - acc: 0.4945 - val_loss: 0.8818 - val_acc: 0.7450\n",
            "Epoch 11/70\n",
            "200/200 [==============================] - 108s 541ms/step - loss: 1.2279 - acc: 0.5192 - val_loss: 0.8397 - val_acc: 0.7544\n",
            "Epoch 12/70\n",
            "200/200 [==============================] - 109s 545ms/step - loss: 1.1614 - acc: 0.5383 - val_loss: 0.8314 - val_acc: 0.7681\n",
            "Epoch 13/70\n",
            "200/200 [==============================] - 109s 545ms/step - loss: 1.1123 - acc: 0.5539 - val_loss: 0.8363 - val_acc: 0.7538\n",
            "Epoch 14/70\n",
            "200/200 [==============================] - 109s 545ms/step - loss: 1.0677 - acc: 0.5663 - val_loss: 0.7614 - val_acc: 0.7556\n",
            "Epoch 15/70\n",
            "200/200 [==============================] - 109s 547ms/step - loss: 1.0238 - acc: 0.5827 - val_loss: 0.7390 - val_acc: 0.7612\n",
            "Epoch 16/70\n",
            "200/200 [==============================] - 109s 544ms/step - loss: 0.9881 - acc: 0.6014 - val_loss: 0.7095 - val_acc: 0.7788\n",
            "Epoch 17/70\n",
            "200/200 [==============================] - 110s 548ms/step - loss: 0.9593 - acc: 0.6034 - val_loss: 0.7179 - val_acc: 0.7756\n",
            "Epoch 18/70\n",
            "200/200 [==============================] - 109s 547ms/step - loss: 0.9216 - acc: 0.6150 - val_loss: 0.7230 - val_acc: 0.7550\n",
            "Epoch 19/70\n",
            "200/200 [==============================] - 109s 547ms/step - loss: 0.8950 - acc: 0.6162 - val_loss: 0.6662 - val_acc: 0.7931\n",
            "Epoch 20/70\n",
            "200/200 [==============================] - 109s 544ms/step - loss: 0.8576 - acc: 0.6305 - val_loss: 0.6776 - val_acc: 0.7706\n",
            "Epoch 21/70\n",
            "200/200 [==============================] - 108s 539ms/step - loss: 0.8554 - acc: 0.6270 - val_loss: 0.6992 - val_acc: 0.7900\n",
            "Epoch 22/70\n",
            "200/200 [==============================] - 107s 537ms/step - loss: 0.8290 - acc: 0.6425 - val_loss: 0.6605 - val_acc: 0.8019\n",
            "Epoch 23/70\n",
            "200/200 [==============================] - 108s 540ms/step - loss: 0.8102 - acc: 0.6358 - val_loss: 0.6520 - val_acc: 0.7981\n",
            "Epoch 24/70\n",
            "200/200 [==============================] - 108s 541ms/step - loss: 0.8102 - acc: 0.6503 - val_loss: 0.6360 - val_acc: 0.7950\n",
            "Epoch 25/70\n",
            "200/200 [==============================] - 108s 540ms/step - loss: 0.7974 - acc: 0.6523 - val_loss: 0.6325 - val_acc: 0.8087\n",
            "Epoch 26/70\n",
            "200/200 [==============================] - 109s 543ms/step - loss: 0.7865 - acc: 0.6530 - val_loss: 0.6304 - val_acc: 0.8137\n",
            "Epoch 27/70\n",
            "200/200 [==============================] - 109s 544ms/step - loss: 0.7714 - acc: 0.6539 - val_loss: 0.6559 - val_acc: 0.7863\n",
            "Epoch 28/70\n",
            "200/200 [==============================] - 109s 544ms/step - loss: 0.7629 - acc: 0.6659 - val_loss: 0.6258 - val_acc: 0.8156\n",
            "Epoch 29/70\n",
            "200/200 [==============================] - 109s 546ms/step - loss: 0.7636 - acc: 0.6597 - val_loss: 0.6365 - val_acc: 0.8144\n",
            "Epoch 30/70\n",
            "200/200 [==============================] - 108s 540ms/step - loss: 0.7663 - acc: 0.6623 - val_loss: 0.6251 - val_acc: 0.8237\n",
            "Epoch 31/70\n",
            "200/200 [==============================] - 107s 537ms/step - loss: 0.7516 - acc: 0.6620 - val_loss: 0.6480 - val_acc: 0.8281\n",
            "Epoch 32/70\n",
            "200/200 [==============================] - 108s 541ms/step - loss: 0.7495 - acc: 0.6722 - val_loss: 0.6406 - val_acc: 0.8231\n",
            "Epoch 33/70\n",
            "200/200 [==============================] - 108s 539ms/step - loss: 0.7209 - acc: 0.6780 - val_loss: 0.6121 - val_acc: 0.8137\n",
            "Epoch 34/70\n",
            "200/200 [==============================] - 108s 542ms/step - loss: 0.7360 - acc: 0.6789 - val_loss: 0.6252 - val_acc: 0.8225\n",
            "Epoch 35/70\n",
            "200/200 [==============================] - 108s 542ms/step - loss: 0.7317 - acc: 0.6814 - val_loss: 0.6047 - val_acc: 0.8225\n",
            "Epoch 36/70\n",
            "200/200 [==============================] - 108s 541ms/step - loss: 0.7155 - acc: 0.6870 - val_loss: 0.6046 - val_acc: 0.8125\n",
            "Epoch 37/70\n",
            "200/200 [==============================] - 108s 542ms/step - loss: 0.7168 - acc: 0.6898 - val_loss: 0.6097 - val_acc: 0.8156\n",
            "Epoch 38/70\n",
            "200/200 [==============================] - 107s 537ms/step - loss: 0.7081 - acc: 0.6977 - val_loss: 0.5875 - val_acc: 0.8363\n",
            "Epoch 39/70\n",
            "200/200 [==============================] - 109s 543ms/step - loss: 0.6781 - acc: 0.7056 - val_loss: 0.6040 - val_acc: 0.8281\n",
            "Epoch 40/70\n",
            "200/200 [==============================] - 108s 539ms/step - loss: 0.6669 - acc: 0.7095 - val_loss: 0.5936 - val_acc: 0.8394\n",
            "Epoch 41/70\n",
            "200/200 [==============================] - 108s 539ms/step - loss: 0.6609 - acc: 0.7091 - val_loss: 0.6194 - val_acc: 0.8325\n",
            "Epoch 42/70\n",
            "200/200 [==============================] - 108s 538ms/step - loss: 0.6744 - acc: 0.7106 - val_loss: 0.5970 - val_acc: 0.8387\n",
            "Epoch 43/70\n",
            "200/200 [==============================] - 108s 538ms/step - loss: 0.6654 - acc: 0.7050 - val_loss: 0.5960 - val_acc: 0.8281\n",
            "Epoch 44/70\n",
            "200/200 [==============================] - 107s 536ms/step - loss: 0.6494 - acc: 0.7184 - val_loss: 0.6116 - val_acc: 0.8287\n",
            "Epoch 45/70\n",
            "200/200 [==============================] - 108s 538ms/step - loss: 0.6546 - acc: 0.7137 - val_loss: 0.6094 - val_acc: 0.8375\n",
            "Epoch 46/70\n",
            "200/200 [==============================] - 108s 538ms/step - loss: 0.6586 - acc: 0.7145 - val_loss: 0.6327 - val_acc: 0.8294\n",
            "Epoch 47/70\n",
            "200/200 [==============================] - 108s 539ms/step - loss: 0.6387 - acc: 0.7192 - val_loss: 0.6092 - val_acc: 0.8287\n",
            "Epoch 48/70\n",
            "200/200 [==============================] - 109s 547ms/step - loss: 0.6360 - acc: 0.7222 - val_loss: 0.6027 - val_acc: 0.8350\n",
            "Epoch 49/70\n",
            "200/200 [==============================] - 107s 536ms/step - loss: 0.6283 - acc: 0.7212 - val_loss: 0.6188 - val_acc: 0.8200\n",
            "Epoch 50/70\n",
            "200/200 [==============================] - 108s 540ms/step - loss: 0.6279 - acc: 0.7342 - val_loss: 0.5832 - val_acc: 0.8325\n",
            "Epoch 51/70\n",
            "200/200 [==============================] - 108s 538ms/step - loss: 0.6291 - acc: 0.7292 - val_loss: 0.5949 - val_acc: 0.8337\n",
            "Epoch 52/70\n",
            "200/200 [==============================] - 108s 540ms/step - loss: 0.6306 - acc: 0.7294 - val_loss: 0.5855 - val_acc: 0.8344\n",
            "Epoch 53/70\n",
            "200/200 [==============================] - 108s 542ms/step - loss: 0.6289 - acc: 0.7328 - val_loss: 0.6113 - val_acc: 0.8331\n",
            "Epoch 54/70\n",
            "200/200 [==============================] - 108s 539ms/step - loss: 0.6256 - acc: 0.7300 - val_loss: 0.6009 - val_acc: 0.8369\n",
            "Epoch 55/70\n",
            "200/200 [==============================] - 107s 535ms/step - loss: 0.6219 - acc: 0.7339 - val_loss: 0.5883 - val_acc: 0.8369\n",
            "Epoch 56/70\n",
            "200/200 [==============================] - 107s 537ms/step - loss: 0.6097 - acc: 0.7405 - val_loss: 0.6283 - val_acc: 0.8275\n",
            "Epoch 57/70\n",
            "200/200 [==============================] - 108s 538ms/step - loss: 0.6016 - acc: 0.7473 - val_loss: 0.6059 - val_acc: 0.8069\n",
            "Epoch 58/70\n",
            "200/200 [==============================] - 107s 536ms/step - loss: 0.5973 - acc: 0.7405 - val_loss: 0.5912 - val_acc: 0.8287\n",
            "Epoch 59/70\n",
            "200/200 [==============================] - 107s 535ms/step - loss: 0.6159 - acc: 0.7397 - val_loss: 0.5795 - val_acc: 0.8300\n",
            "Epoch 60/70\n",
            "200/200 [==============================] - 108s 539ms/step - loss: 0.5993 - acc: 0.7444 - val_loss: 0.5877 - val_acc: 0.8350\n",
            "Epoch 61/70\n",
            "200/200 [==============================] - 108s 542ms/step - loss: 0.5993 - acc: 0.7434 - val_loss: 0.6117 - val_acc: 0.8350\n",
            "Epoch 62/70\n",
            "200/200 [==============================] - 108s 542ms/step - loss: 0.5811 - acc: 0.7547 - val_loss: 0.6173 - val_acc: 0.8256\n",
            "Epoch 63/70\n",
            "200/200 [==============================] - 111s 555ms/step - loss: 0.5720 - acc: 0.7569 - val_loss: 0.6331 - val_acc: 0.8256\n",
            "Epoch 64/70\n",
            "200/200 [==============================] - 113s 564ms/step - loss: 0.5749 - acc: 0.7602 - val_loss: 0.6447 - val_acc: 0.8181\n",
            "Epoch 65/70\n",
            "200/200 [==============================] - 113s 563ms/step - loss: 0.5704 - acc: 0.7555 - val_loss: 0.5824 - val_acc: 0.8313\n",
            "Epoch 66/70\n",
            "200/200 [==============================] - 115s 574ms/step - loss: 0.5828 - acc: 0.7427 - val_loss: 0.6423 - val_acc: 0.8237\n",
            "Epoch 67/70\n",
            "200/200 [==============================] - 113s 567ms/step - loss: 0.5770 - acc: 0.7542 - val_loss: 0.6060 - val_acc: 0.8281\n",
            "Epoch 68/70\n",
            "200/200 [==============================] - 114s 572ms/step - loss: 0.5662 - acc: 0.7528 - val_loss: 0.6388 - val_acc: 0.8206\n",
            "Epoch 69/70\n",
            "200/200 [==============================] - 114s 571ms/step - loss: 0.5481 - acc: 0.7683 - val_loss: 0.6647 - val_acc: 0.8281\n",
            "Epoch 70/70\n",
            "200/200 [==============================] - 112s 560ms/step - loss: 0.5685 - acc: 0.7639 - val_loss: 0.6136 - val_acc: 0.8206\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFxEpE0roIiJl4v8D1dFxdmGbEqAAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}